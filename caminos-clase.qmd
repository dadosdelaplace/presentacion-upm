---
title: "Concurso de plaza de Ayudante Doctor"
subtitle: "Exposici√≥n docente de Javier √Ålvarez Li√©bana"
author: "Departamento de Matem√°tica e Inform√°tica aplicadas a las Ingenier√≠as Civil y Naval"
lang: es
format: 
  revealjs:
    embed-resources: true
    theme: [default, style2.scss]
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier √Ålvarez Li√©bana</strong>](...) ‚Ä¢ Concurso de plaza Ayudante Doctor en la UPM"
    slide-number: c/t
    width: 1120
execute:
  echo: true
---

## Concurso de plaza

![](exposicion-docente.jpg){width=200px}

[**Exposici√≥n docente**]{.hl-orange}: 

  - Adecuaci√≥n a la lecci√≥n al perfil de la plaza
  - Claridad en la explicaci√≥n
  - Adaptaci√≥n al tiempo disponible
  - Respuesta a las dudas y cuestiones del tribunal 
 
---

## Asignaturas del perfil

::: {.smaller-un-poquito-text}

:::: columns
::: {.column width="33%"}

#### Inform√°tica

* Entorno de Computaci√≥n
* Generaci√≥n de vectores y matrices. Operaciones con tablas
* Bifurcaciones y bucles
* Funciones definidas por el usuario
* Entrada y salida de datos
* Herramientas de Visualizaci√≥n
* Matrices y Sistemas Lineales. Normas matriciales

:::

::: {.column width="33%"}

#### Ciencia de datos

* Entrenamiento, validaci√≥n y sobreajuste de m√°quinas para clasificaci√≥n, regresi√≥n y predicci√≥n. Entrenamiento supervisado y no supervisado
* Redes neuronales artificiales
* Interpretaci√≥n y an√°lisis de la sensibilidad
* T√©cnicas de visualizaci√≥n de resultados

:::

::: {.column width="33%"}

#### Estad√≠stica computacional

* Modelos de probabilidad (discretos/continuos)
* An√°lisis de la varianza
* Modelos de regresi√≥n y clasificaci√≥n
* Modelos para reducir la dimensionalidad: Componentes principales y regresi√≥n de m√≠nimos cuadrados parciales
* Series temporales. Modelos ARIMA
* M√©todos de Montecarlo


:::

::::
:::

---

## Asignaturas del perfil

::: {.smaller-un-poquito-text}

:::: columns
::: {.column width="33%"}

#### Inform√°tica

* Entorno de Computaci√≥n
* <span style="color:#f39b4f">Generaci√≥n de vectores y matrices. Operaciones con tablas</span>
* Bifurcaciones y bucles
* Funciones definidas por el usuario
* <span style="color:#f39b4f">Entrada y salida de datos</span>
* <span style="color:#f39b4f">Herramientas de Visualizaci√≥n</span>
* <span style="color:#f39b4f">Matrices y Sistemas Lineales. Normas matriciales</span>

:::

::: {.column width="33%"}

#### Ciencia de datos

* <span style="color:#f39b4f">Entrenamiento, validaci√≥n y sobreajuste de m√°quinas para clasificaci√≥n, regresi√≥n y predicci√≥n. Entrenamiento supervisado</span> y no supervisado
* Redes neuronales artificiales
* Interpretaci√≥n y an√°lisis de la sensibilidad
* <span style="color:#f39b4f">T√©cnicas de visualizaci√≥n de resultados</span>

:::

::: {.column width="33%"}

#### Estad√≠stica computacional

* <span style="color:#f39b4f">Modelos de probabilidad (discretos/continuos)</span>
* <span style="color:#f39b4f">An√°lisis de la varianza</span>
* <span style="color:#f39b4f">Modelos de regresi√≥n</span> y clasificaci√≥n
* Modelos para reducir la dimensionalidad: Componentes principales y regresi√≥n de m√≠nimos cuadrados parciales
* Series temporales. Modelos ARIMA
* <span style="color:#f39b4f">M√©todos de Montecarlo</span>


:::

::::
:::

Realizar√© una exposici√≥n docente en torno a [**modelos lineales supervisados**]{.hl-orange}


---

## Material docente



* Estas diapositivas se han elaborado integramente con [**Quarto**]{.hl-orange}, una herramienta de Posit (antiguo RStudio) tremendamente potente para **comunicar y combinar textos, f√≥rmulas, im√°genes, gr√°ficos y c√≥digo** (admite c√≥digo R, Python, Matlab, C, etc)

::: {.fragment}

* Se ha optado por el [**uso de R para el c√≥digo**]{.hl-orange} cuyas cajas los alumnos pueden copiar y pegar

```{r}
x <- 1
x
```

:::

::: {.fragment}

* El [**c√≥digo de las diapositivas**]{.hl-orange} se encuentran disponibles libremente en Github y en el contenido se puede seguir en el [**enlace**]{.hl-orange} <https://javieralvarezliebana.es/exposicion-docente-caminos.html>

:::

::: {.smaller-not-so-text}

::: {.fragment}

üìö Alguna bibliograf√≠a b√°sica:

[1] ¬´The elements of Statistical Learning¬ª (Hastie et al., 2008)

[2] ¬´Regression And Other Stories¬ª (Gelman, Hill and Vehtari, 2020).

[3] ¬´Elements of Statistical Learning¬ª (John Ross Quinlan, 1986).

[4] ¬´Mathematical Statistics and Data Analysis¬ª (John A. Rice, 2010).

:::
:::

---

## Objetivos

::: columns
::: {.column width="20%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png){width=230px}

:::

::: {.column width="80%"}
::: incremental
- [**Relacionar competencias**]{.hl-orange} de probabilidad, modelizaci√≥n lineal, simulaci√≥n, inferencia estad√≠stica, programaci√≥n y visualizaci√≥n de datos.

- Aprender los fundamentos del [**aprendizaje estad√≠stico**]{.hl-orange}

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos (lineales)**]{.hl-orange}

- Entender en profundidad el contexto de la [**predicci√≥n lineal**]{.hl-orange}.


:::
:::
:::


::: {.fragment}

### Requisitos

1.  [**Conexi√≥n a internet**]{.hl-orange} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-orange}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Tener cursadas asignaturas b√°sica de estad√≠stica y probabilidad, geometr√≠a y √°lgebra.

:::

---


## ¬øQu√© es predecir?

En esta asignatura vamos a tratar principalmente lo que se conoce en el aprendizaje estad√≠stico como [**predicci√≥n (continua)**]{.hl-orange}. Dada una [**variable objetivo (variable dependiente)**]{.hl-orange}, y con la informaci√≥n aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-orange}, el objetivo ser√° obtener una estimaci√≥n/predicci√≥n lo ¬´mejor posible¬ª haciendo uso de un

. . .

&nbsp;


[**modelo de APRENDIZAJE**]{.hl-orange} [**SUPERVISADO**]{.hl-blue} de [**PREDICCI√ìN**]{.hl-purple} [**LINEAL**]{.hl-green} (conocido como regresi√≥n lineal)


---


## ¬øModelo de aprendizaje?


[**modelo de APRENDIZAJE**]{.hl-orange} supervisado de predicci√≥n lineal

&nbsp;

:::: columns
::: {.column width="40%"}

En esta asignatura veremos el modelo m√°s simple de lo que se conoce como [**aprendizaje estad√≠stico (Machine Learning)**]{.hl-orange}: modelos que, tras un n√∫mero de iteraciones, son capaces de ¬´aprender¬ª de los datos (en realidad es un proceso de optimizaci√≥n que hemos ¬´humanizado¬ª)

:::

::: {.column width="60%"}

![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/ml.jpg?raw=true)

:::
::::


---


## ¬øSupervisado?

En el campo del Machine Learning hay principalmente dos tipos de modelos:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-blue}: tendremos dos tipos de variables, la [**variable dependiente (output)**]{.hl-orange} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes o predictoras**]{.hl-orange}, que contienen la informaci√≥n disponible. Ejemplos: regresi√≥n, knn, √°rboles, etc.
 

:::

::: {.column width="50%"}

![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/supervised.jpg?raw=true)

:::
::::

::: {.fragment}

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-blue}: no existe la distinci√≥n entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-orange}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscar√° patrones**]{.hl-orange} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/unsupervised.jpg?raw=true)

:::
::::

:::

---


## ¬øPredicci√≥n?

modelo de aprendizaje supervisado de [**PREDICCI√ìN**]{.hl-purple} lineal

&nbsp;

La **regresi√≥n lineal** se enmarca dentro de la [**predicci√≥n**]{.hl-purple} supervisada

:::: columns
::: {.column width="50%"}

![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/vitro-fuego-discretas.jpg?raw=true)

:::

::: {.column width="50%"}

* [**Predicci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-purple} (por ejemplo, precio, glucosa, peso, temperatura, volumen, etc).

* [**Clasificaci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-purple} (por ejemplo, categor√≠a, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, n¬∫ de personas), pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

:::
::::


---

## ¬øLineal?


modelo de aprendizaje supervisado de predicci√≥n [**LINEAL**]{.hl-green}

&nbsp;

En matem√°ticas decimos que una funci√≥n $f(x)$ es [**lineal**]{.hl-green} cuando se cumple:

* [**Propiedad aditiva**]{.hl-green}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homog√©nea**]{.hl-green}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

Llamaremos [**modelo de predicci√≥n lineal**]{.hl-green} a un modelo que usa la informaci√≥n de covariables $X_1, X_2, \ldots, X_p$, de manera que su informaci√≥n siempre [**se relacionen entre s√≠ con sumas y restas**]{.hl-orange}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$



---


## Modelo de regresi√≥n lineal

El [**modelo te√≥rico de predicci√≥n lineal**]{.hl-orange} se basa en:

$$Y = f \left(X_1, \ldots, X_p \right) + \varepsilon = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p  + \varepsilon$$

::: {.fragment}

:::: columns
::: {.column width="60%"}

Supongamos por ejemplo que queremos estimar $Y = peso$ en funci√≥n de $X_1 = estatura$ y $X_2 = edad$, tal que $peso = \beta_0 + \beta_1*estatura + \beta_2* edad + \varepsilon$

:::

::: {.column width="40%"}

```{r}
#| echo: false
library(dplyr)
starwars |> 
  select(name, height, mass, birth_year) |> 
  rename(age = birth_year) |> 
  slice(1:4)
```

:::
::::

:::

::: incremental

* ¬øPor qu√© es un [**modelo de aprendizaje**]{.hl-orange}? Porque con la info de $\left(X_1, \ldots, X_p \right)$  que se le vaya proporcionando ir√° ajustando su estimaci√≥n de $Y$  (que denotaremos como $\widehat{Y}$)

* ¬øPor qu√© [**supervisado**]{.hl-blue}? Porque de cada individuo conozco el valor real de $Y$ (y una vez obtenida la estimaci√≥n $\widehat{Y}$ podr√© supervisar lo que me equivoco)

* ¬øPor qu√© [**predicci√≥n**]{.hl-purple}? Porque $Y$ es una variable continua

* ¬øPor qu√© [**lineal**]{.hl-green}? Porque estatura y edad solo les acompa√±an constantes y sumas/restas: no se multiplican/dividen entre s√≠, no hay logaritmo, seno, coseno, etc.


:::

---

## Modelo de regresi√≥n lineal

$$Y = f \left(X_1, \ldots, X_p \right) + \varepsilon = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p  + \varepsilon$$


* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ ser√°n los [**datos**]{.hl-orange}, nuestras variables predictoras.

* $f(\cdot)$ ser√° nuestro [**modelo**]{.hl-orange} (con la informaci√≥n $\mathbf{X}$), en este caso, recta o hiperplano lineal.

* $\varepsilon$ ser√° el [**error o ruido**]{.hl-orange}, una [**variable aleatoria de media 0**]{.hl-orange}: aleatorio y desconocido, lo que no podemos controlar (si no existiese ese error -> asignatura de an√°lisis)

::: incremental


* El modelo lineal solo busca [**modelizar la parte determin√≠stica**]{.hl-orange}, es decir, busca modelizar $E \left[Y | \boldsymbol{X} \right]$ como $\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$ (la media del error es nula)

* Esa esperanza es el valor t√©orico suponiendo que [**conoci√©semos la poblaci√≥n entera**]{.hl-orange} (por ejemplo, conocer la estatura y edad de TODA la poblaci√≥n). [**¬øEs eso posible?**]{.hl-red}

:::

---

## Muestra vs poblaci√≥n

-   [**Poblaci√≥n**]{.hl-orange}

La poblaci√≥n es el [**conjunto completo de elementos o individuos**]{.hl-orange}. En algunos casos es posible acceder a ella (mediante **censos**) pero la mayor√≠a de casos el [**acceso a la totalidad de la poblaci√≥n es inviable**]{.hl-orange} por motivos econ√≥micos, legales o √©ticos.

&nbsp;

. . .


- [**Muestra**]{.hl-orange}: subconjunto de tama√±o $n$ de la poblaci√≥n que se selecciona para su an√°lisis, con el fin de hacer generalizaciones (inferencia) sobre la poblaci√≥n completa, a trav√©s de **encuestas, experimentos, observaciones, registros**, etc. 

  -   **Muestreo aleatorio simple**: cada individuo tiene la misma probabilidad de ser seleccionado.
  -   **Muestreo estratificado**: la poblaci√≥n se divide en subgrupos (estratos) y se toma una muestra de cada uno (por ejemplo, mismo % por sexos).
  -   **Otros**: muestreo (no aleatorio) por cuotas, muestreo por conveniencia, etc.

---

## Muestra vs poblaci√≥n

La rama de la estad√≠stica dedicada a ello se conoce como [**muestreo**]{.hl-orange}, y aunque no es el objetivo de esta asignatura, es fundamental reflexionar sobre c√≥mo se han recopilado los datos.

![](https://sketchplanations.com/_next/image?url=https%3A%2F%2Fimages.prismic.io%2Fsketchplanations%2Ff2fdb7cb-f126-4897-ad78-4fd11c743172_SP%2B723%2B-%2BSampling%2Bbias.png%3Fauto%3Dcompress%2Cformat&w=828&q=75)

[**Sesgo de selecci√≥n**]{.hl-orange}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

---


## Modelo muestral

:::: columns
::: {.column width="48.5%"}

#### Te√≥rico (poblacional)

::: {.smaller-papers-text}


$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p  + \varepsilon$$

$$ $$

:::

::: {.smaller-muestra-poblacion-text}

* $X_1, \ldots, X_p$ [**predictoras (aleatorias)**]{.hl-orange}: estatura y edad de TODA la poblaci√≥n.

* $Y$ [**objetivo (aleatoria)**]{.hl-orange}: peso de TODA la poblaci√≥n.


:::

:::

::: {.column width="3%"}

:::

::: {.column width="48.5%"}

#### Muestral

::: {.smaller-papers-text}

$$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}  + \varepsilon_i \quad \text{te√≥rico}$$

$$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} + \ldots + \widehat{\beta}_p x_{ip} \quad \text{estimado}$$

:::

::: {.smaller-muestra-poblacion-text}

* $\left(x_{i1}, \ldots, x_{ip}\right)$ son [**datos en una tabla**]{.hl-orange} para el individuo i-√©simo: estatura y edad del individuo i.
* $y_i$ es el [**valor OBSERVADO**]{.hl-orange}: peso del individuo i.


:::
:::

::::


---


## Modelo muestral

:::: columns
::: {.column width="48.5%"}

#### Te√≥rico (poblacional)

::: {.smaller-papers-text}


$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p  + \varepsilon$$

$$ $$

:::

::: {.smaller-muestra-poblacion-text}

* $X_1, \ldots, X_p$ predictoras (aleatorias): estatura y edad de TODA la poblaci√≥n.

* $Y$ objetivo (aleatoria): peso de TODA la poblaci√≥n.

* $\left(\beta_0, \ldots, \beta_p \right)$ [**fijas**]{.hl-orange} pero desconocidas.


:::

:::

::: {.column width="3%"}

:::

::: {.column width="48.5%"}

#### Muestral

::: {.smaller-papers-text}

$$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}  + \varepsilon_i \quad \text{te√≥rico}$$

$$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} + \ldots + \widehat{\beta}_p x_{ip} \quad \text{estimado}$$

:::

::: {.smaller-muestra-poblacion-text}

* $\left(x_{i1}, \ldots, x_{ip}\right)$ son datos en una tabla  para el individuo i-√©simo: estatura y edad del individuo i.
* $y_i$ es el valor OBSERVADO: peso del individuo i.

* $\left(\widehat{\beta}_0, \ldots, \widehat{\beta}_p\right)$ [**variables aleatorias**]{.hl-orange} (dependen de la muestra).


:::
:::

::::


---


## Modelo muestral

:::: columns
::: {.column width="48.5%"}

#### Te√≥rico (poblacional)

::: {.smaller-papers-text}


$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p  + \varepsilon$$

$$ $$

:::

::: {.smaller-muestra-poblacion-text}

* $X_1, \ldots, X_p$ predictoras (aleatorias): estatura y edad de TODA la poblaci√≥n.

* $Y$ objetivo (aleatoria): peso de TODA la poblaci√≥n.

* $\left(\beta_0, \ldots, \beta_p \right)$ fijas pero desconocidas.

* $\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$ [**modelo te√≥rico que pretende explicar el valor promedio**]{.hl-orange} de $Y$ con la info disponible de $X$ ($E[Y|X]$): posible relaci√≥n biol√≥gica entre estatura, peso y edad (en promedio).


:::

:::

::: {.column width="3%"}

:::

::: {.column width="48.5%"}

#### Muestral

::: {.smaller-papers-text}

$$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}  + \varepsilon_i \quad \text{te√≥rico}$$

$$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} + \ldots + \widehat{\beta}_p x_{ip} \quad \text{estimado}$$

:::

::: {.smaller-muestra-poblacion-text}

* $\left(x_{i1}, \ldots, x_{ip}\right)$ son datos en una tabla para el individuo i-√©simo: estatura y edad del individuo i.

* $y_i$ es el valor OBSERVADO: peso del individuo i.

* $\left(\widehat{\beta}_0, \ldots, \widehat{\beta}_p\right)$ variables aleatorias (dependen de la muestra).

* $\widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} + \ldots + \widehat{\beta}_p x_{ip}$ la [**estimaci√≥n que hacemos del modelo real**]{.hl-orange} con la informaci√≥n de la muestra. 


:::
:::

::::


---

## Modelo muestral

:::: columns
::: {.column width="48.5%"}

#### Te√≥rico (poblacional)

::: {.smaller-papers-text}


$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p  + \varepsilon$$

$$ $$

:::

::: {.smaller-muestra-poblacion-text}

* $X_1, \ldots, X_p$ predictoras (aleatorias): estatura y edad de TODA la poblaci√≥n.

* $Y$ objetivo (aleatoria): peso de TODA la poblaci√≥n.

* $\left(\beta_0, \ldots, \beta_p \right)$ fijas pero desconocidas.

* $\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$ modelo te√≥rico que pretende explicar el valor promedio de $Y$ con la info disponible de $X$ ($E[Y|X]$): posible relaci√≥n biol√≥gica entre estatura, peso y edad (en promedio).

* $\varepsilon$ [**variable aleatoria**]{.hl-orange} entendida como la [**desviaci√≥n individual**]{.hl-orange} del peso de cada persona respecto a esa media te√≥rica (variabilidad gen√©tica) cuyo promedio es nulo  $E[\varepsilon|X] = 0$.

:::

:::

::: {.column width="3%"}

:::

::: {.column width="48.5%"}

#### Muestral

::: {.smaller-papers-text}

$$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}  + \varepsilon_i \quad \text{te√≥rico}$$

$$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} + \ldots + \widehat{\beta}_p x_{ip} \quad \text{estimado}$$

:::

::: {.smaller-muestra-poblacion-text}

* $\left(x_{i1}, \ldots, x_{ip}\right)$ son datos en una tabla para el individuo i-√©simo: estatura y edad del individuo i.
* $y_i$ es el valor OBSERVADO: peso del individuo i.

* $\left(\widehat{\beta}_0, \ldots, \widehat{\beta}_p\right)$ variables aleatorias (dependen de la muestra).

* $\widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} + \ldots + \widehat{\beta}_p x_{ip}$ la estimaci√≥n que hacemos del modelo real con la informaci√≥n de la muestra. 

* $\varepsilon_i$ error que conocer√≠amos si supi√©semos el valor real de $\left(\beta_0, \ldots, \beta_p \right)$. Si solo conocemos sus [**estimaciones muestrales**]{.hl-orange} tendremos una desviaci√≥n estimada $\widehat{\varepsilon}_i$, con media muestral nula.

:::
:::

::::



---

## Modelo muestral


Si queremos predecir peso en funci√≥n de estatura y edad ($p = 2$) de $n=5$ individuos


```{r}
#| code-fold: true
library(dplyr)
starwars |> 
  select(name, height, mass, birth_year) |> 
  rename(age = birth_year) |> 
  slice(1:5)
```

::: incremental

* $x_{i1}$ es 172cm en el caso de Luke Skywalker ($i = 1$) y 96cm en el caso de R2-D2 ($i = 3$)
* $x_{i2}$ es 41.9 a√±os en el caso de Darth Vader ($i = 4$) y 112 a√±os en el caso de C-3PO ($i = 2$)
* $y_{i}$ es 77kg en el caso de Luke Skywalker ($i = 1$) y 49kg en el caso de Leia Organa ($i = 5$)

:::

. . .

¬øObjetivo? Con la [**info de la muestra**]{.hl-orange} (representativa) ser capaces de [**entrenar un modelo**]{.hl-orange} (en este caso, obtener los ¬´mejores¬ª $\left(\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\beta}_2\right)$ que podamos luego [**extrapolar (INFERIR)**]{.hl-orange} a toda la poblaci√≥n (para usarlo con un individuo nuevo que no tengamos en la muestra)


---

## Modelo de regresi√≥n lineal

:::: columns
::: {.column width="40%"}

#### Te√≥rico (poblacional)

$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p  + \varepsilon$$

:::

::: {.column width="4%"}

:::

::: {.column width="56%"}

#### Muestral

$$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}  + \varepsilon_i$$


:::

::::


El anterior [**modelo de regresi√≥n lineal es par√°metrico**]{.hl-orange}: para estimar el valor de $y$ basta con proporcionar una [**estimaci√≥n de los par√°metros**]{.hl-orange} 

::: {.fragment}

$$\widehat{y}_i = \widehat{\beta}_0 +  \widehat{\beta}_1 x_{i1} + \ldots +  \widehat{\beta}_p x_{ip},  \quad \left( \widehat{\beta}_0, \ldots,  \widehat{\beta}_p \right) \quad \text{variables aleatorias}$$

:::

::: {.fragment}

$$\widehat{\varepsilon}_i =  \widehat{y}_i - y_i = \left(\widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} + \ldots + \widehat{\beta}_p x_{ip} \right) - y_i = \text{estimaci√≥n} - \text{realidad}$$


:::

&nbsp;

::: {.fragment}

Buscamos un [**modelo basado en $X$ que explique al m√°ximo lo que sucede en $Y$**]{.hl-orange}, es decir, buscamos un modelo donde el residuo $\widehat{\varepsilon}$ [**contenga la MENOR INFORMACI√ìN posible**]{.hl-orange}

:::

---

## Regresi√≥n univariante

Vamos a simplificar a√∫n m√°s la idea empezando con la [**regresi√≥n univariante**]{.hl-orange} (solo $p = 1$)

```{r}
#| echo: false
library(dplyr)
starwars |> 
  select(name, height, mass) |>
  slice(1:5)
```

¬øC√≥mo predecir el peso ($Y$) a trav√©s de la estatura ($X$)?

. . .


$$Y = \beta_0 + \beta_1 X_1 + \varepsilon, \quad y_i = \beta_0 + \beta_1 x_{i1}  + \varepsilon_i,  \quad \widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i1}$$



* $x_{i1}$ estatura del individuo i-√©simo. $y_i$ peso del individuo i-√©simo
* $\widehat{y}_i$ estimaci√≥n del peso del individuo i-√©simo
* El modelo $\beta_0 + \beta_1 X_1$ representa lo que se conoce como [**recta de regresi√≥n**]{.hl-orange}: la recta que modeliza la relaci√≥n determin√≠stica entre $Y$ y $X$ (m√°s all√° de las desvaciones aleatorias)
* $\widehat{\beta}_0 + \widehat{\beta}_1 X_1$ representa la recta de regresi√≥n estimada

---

## Regresi√≥n univariante


```{r}
#| code-fold: true
library(dplyr)
starwars |> 
  select(name, height, mass) |>
  slice(1:5)
```


$$Y = \beta_0 + \beta_1 X_1 + \varepsilon, \quad y_i = \beta_0 + \beta_1 x_{i1}  + \varepsilon_i, \quad \widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i1}$$

Si la recta de regresi√≥n se estima como $\widehat{\beta}_0 = -70$ y $\widehat{\beta}_1 = 0.87$. [**¬øCu√°l es la estimaci√≥n?**]{.hl-orange}

. . .

$$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i1} = -70 + 0.87*\text{estatura}_i$$

. . .

Por ejemplo, para Luke Skywalker, la predicci√≥n del peso seg√∫n el modelo es de $\widehat{y}_i = 79.64kg$ mientras que su peso real es de $y_i = 77kg$, por lo que la estimaci√≥n del error es $\widehat{\varepsilon}_i = 2.64$


---

## ¬øPor qu√© regresi√≥n?

> Hay una regla universal: cualquier pariente tuyo es probablemente m√°s mediocre que t√∫¬ª

La [**historia de la recta regresi√≥n**]{.hl-orange} se remonta a **Francis Galton**, primo de Charles Darwin, fascinado por ¬´El origen de la especies¬ª: [**Galton no se centraba en los mejor adaptados sino en los que √©l llama mediocres**]{.hl-orange}. Seg√∫n √©l las sociedades estaban fomentando la mediocridad  as√≠ que empez√≥ a estudiar la [**relaci√≥n de la estatura de los hijos respecto a los padres**]{.hl-orange}

. . .

En 1886 public√≥ ¬´Regression towards mediocrity in hereditary stature¬ª, un art√≠culo que cambiar√≠a la estad√≠stica: fue el [**primer uso documentado de la recta de regresi√≥n**]{.hl-orange} 

. . .

:::: columns
::: {.column width="30%"}

![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/reg_dalton.jpg?raw=true)

:::

::: {.column width="70%"}

Galton analiz√≥ la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones hab√≠a una [**REGRESI√ìN (un retroceso) a la mediocridad (entendida como la media)**]{.hl-orange} 


:::
::::

Hijos de altos eran algo m√°s bajitos, e hijos de bajitos eran algo m√°s altos.

---

## ¬øPor qu√© regresi√≥n?

:::: columns
::: {.column width="60%"}

![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/reg-galton-coef.jpg?raw=true)

:::

::: {.column width="40%"}

Galton no solo observ√≥ que las [**estaturas ¬´regresaban¬ª a un valor medio sino que lo hac√≠an con un patr√≥n**]{.hl-orange}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## ¬øC√≥mo determinar la recta?

Ya sabemos que si tenemos una variable $Y$ que queremos explicar en funci√≥n $X$, el modelo lineal (regresi√≥n univariante) nos dice que basta con encontrar una **estimaci√≥n de los par√°metros de la recta**. ¬øPero... [**cu√°l de todas las rectas posibles**]{.hl-orange}?

. . .

:::: columns
::: {.column width="40%"}

El modelo m√°s sencillo ser√≠a una **recta horizontal** representada por el modelo trivial $\widehat{y} = \widehat{\beta}_0 = \bar{y} = cte$, un modelo que [**no aprende**]{.hl-red} ya que no incorpora informaci√≥n de $X$.

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| fig-width: 8
datos <-
  starwars |> 
  select(name, height, mass) |>
  slice(c(1:15, 17:20)) |> 
  mutate("pred" = mean(mass, na.rm = TRUE))
library(ggplot2)

ggplot(datos, aes(x = height, y = mass)) +
  geom_segment(aes(xend = height, y = pred, yend = mass),
               color = "pink", size = 1) +
  geom_point(color = "orange", size = 4) +
  geom_hline(aes(yintercept = pred), color = "black", size = 1.5) +
  labs(x = "estatura (cm)", y = "peso (kg)") +
  theme_minimal()
```

:::
::::

---


## ¬øC√≥mo determinar la recta?

¬ø[**Cu√°l de todas las rectas posibles**]{.hl-orange}?

:::: columns
::: {.column width="70%"}

![](pendiente_con_barras.gif)

:::

::: {.column width="30%"}

El objetivo ser√° [**explicar el m√°ximo de informaci√≥n (variabilidad) de Y con la informaci√≥n de X**]{.hl-orange}, o lo que es lo mismo, que [**minimice la informaci√≥n que queda sin explicar**]{.hl-orange}: buscamos la recta que, en promedio, minimice las distancias verticales (al cuadrado).


:::
::::


---

## Varianza residual

El objetivo ser√° [**explicar el m√°ximo de informaci√≥n (variabilidad) de Y con la informaci√≥n de X**]{.hl-orange}, o lo que es lo mismo, que [**minimice la informaci√≥n que queda sin explicar**]{.hl-orange}


* La informaci√≥n sin explicar viene capturada en el error estimado $\widehat{\varepsilon}$ (modelo - realidad)
* [**Informaci√≥n en estad√≠stica = varianza**]{.hl-orange}


. . .

De todas las rectas posibles nos quedaremos con la que [**minimice la varianza (muestral) del error**]{.hl-orange} (varianza residual muestral), que se define como sigue (ya que su media es cero)


$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} =  \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## Varianza residual

$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} =  \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$


Esa varianza residual cuantifica la variabilidad (= informaci√≥n) de la variable objetivo que **NO EXPLICA** nuestro modelo sobre X,  [**cuantifica c√∫anto nos equivamos en promedio**]{.hl-orange}. Adem√°s es una funci√≥n que depende de dos par√°metros $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. ¬øC√≥mo [**encontrar el m√≠nimo**]{.hl-orange}?

&nbsp;

. . .

[**Idea**]{.hl-blue}: la forma m√°s directa es por el [**m√©todo de los m√≠nimos cuadrados**]{.hl-orange}, originariamente planteado por Legendre y Gauss para estimar la posici√≥n de los planeta. Los [**par√°metros √≥ptimos**]{.hl-orange} ser√°n aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual), desarrollar el cuadrado, [**derivar respecto a cada variable e igualar a 0**]{.hl-orange}.


---

## Varianza residual

$$s_{r}^{2} =  \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} =  \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

Si derivamos...

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2\beta_0 + 2\beta_1\frac{1}{n} \sum_{i=1}^{n} X_i - 2\frac{1}{n} \sum_{i=1}^{n} Y_i  = 2 \left(\beta_{0} + \beta_1\bar{x} - \bar{y}\right) $$ 

. . .

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] =  2 \left( \beta_{1} \overline{x^2}+\beta_{0}\bar{x} - \overline{xy} \right)$$ 

. . .

Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-orange} para encontrar el √≥ptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \bar{x}   = \bar{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 


---

## M√©todo m√≠nimos cuadrados

Resolviendo el sistema (con Cramer por ejemplo) tenemos

$$\widehat{\beta}_1 = \frac{\overline{xy} - \bar{x}*\bar{y}}{\overline{x^2} - \left(\bar{x}\right)^2} = \frac{s_{xy}}{s_{x}^{2}}, \quad s_{xy}~  \text{ covarianza muestral entre predictora y objetivo}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \bar{x}*\overline{xy}}{\overline{x^2} - \left(\bar{x}\right)^2}  = \bar{y} + \frac{\bar{y}*\left(\bar{x}\right)^2- \bar{x}*\overline{xy}}{\overline{x^2} - \left(\bar{x}\right)^2} = \bar{y} - \widehat{\beta}_1 \bar{x}$$



. . .

Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-orange} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \bar{y} - \widehat{\beta}_1 \bar{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .


* Los par√°metros $\left( \beta_0, \beta_1 \right)$ son [**desconocidos pero fijos**]{.hl-orange}

* Los par√°metros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-orange} ya que han sido calculados en funci√≥n de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la poblaci√≥n.

---

## Estimaci√≥n reg. univariante

Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-orange} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \bar{y} - \widehat{\beta}_1 \bar{x}$ ([**ordenada en el origen**]{.hl-purple}).

En nuestro ejemplo: $peso = -35.195 + 0.6853*estatura$


```{r}
#| echo: false
datos <-
  starwars |> 
  select(name, height, mass) |>
  slice(c(1:15, 17:20)) |> 
  mutate("pred" = -35.19510 + 0.68528*height)
```

```{r}
#| code-fold: true
ggplot(datos, aes(x = height, y = mass)) +
  geom_segment(aes(xend = height, y = pred, yend = mass),
               color = "pink", size = 1) +
  geom_point(color = "orange", size = 4) +
  geom_line(aes(x = height, y = pred), color = "black", size = 1.5) +
  labs(x = "estatura (cm)", y = "peso (kg)") +
  theme_minimal()
```


---


## Estimaci√≥n reg. univariante

Estructuralmente: si unimos con cuerdas cada punto a las posibles rectas, la recta obtenida por m√≠nimos cuadrados es la **recta de equilibrio de ¬´tensar¬ª todos las cuerdas a la vez**.

<video autoplay loop muted playsinline width="600">
  <source src="edKRHenUXKJAGnvb.mp4" type="video/mp4">
</video>


---

## Estimaci√≥n reg. univariante

En nuestro ejemplo: $peso = -35.195 + 0.6853*estatura$. ¬øCu√°l es su [**interpretaci√≥n**]{.hl-orange}?

* [**Ordenada en el origen**]{.hl-orange}: tambi√©n llamado **intercepto** es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimaci√≥n de $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimaci√≥n tenga sentido). En nuestro ejemplo: la estimaci√≥n del peso de cuando no hay estatura es de $-35.195kg$ (sin fiabilidad ya que la estatura nunca valdr√° 0)


. . .

* [**Pendiente**]{.hl-orange}: cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variaci√≥n de la **estimaci√≥n $\widehat{Y}$ cuando $X$ tiene un incremento unitario**. En nuestro ejemplo: en promedio el peso aumenta 0.685kg por cada cm de m√°s que mida el individuo.

&nbsp;

. . .

¬°Enhorabuena! Acabamos de hacer nuestro [**primer modelo de aprendizaje estad√≠stico supervisado**]{.hl-orange}

---


## ¬°Cuidado!

A lo largo del proceso hemos mentado 3 problemas a los que prestar atenci√≥n:

::: incremental

1. ¬øQu√© rol est√° jugando en todo el proceso la [**covarianza entre X e Y**]{.hl-orange}? ¬øQu√© pasa si es cercana a 0?

2. ¬øC√≥mo cuantificar la [**fiabilidad de mi estimaci√≥n**]{.hl-orange}?

3. ¬øC√≥mo garantizar que el [**m√©todo lo podr√≠a extrapolar/inferir**]{.hl-orange} a otros individuos ajenos a la muestra? Si descubro un modelo para predecir la diabetes o el colesterol en 1000 pacientes, ¬øc√≥mo saber que la recomendaci√≥n de hacer ejercicio o dormir bien la puedo generalizar fuera de esa muestra?

:::

---

## Covarianza y correlaci√≥n

1. ¬øQu√© rol est√° jugando en todo el proceso la [**covarianza entre X e Y**]{.hl-orange}? ¬øQu√© pasa si es cercana a 0?

Como hemos visto la [**pendiente de la recta depende de la covarianza**]{.hl-orange}. La [**covarianza**]{.hl-orange} cuantifica la [**asociaci√≥n LINEAL**]{.hl-orange}  de una variable X respecto a otra variable Y (ambas continuas): nos est√° cuantificando c√≥mo de pegados est√°n los puntos a la recta

:::: columns
::: {.column width="23%"}

$$s_{xy} = \overline{x*y} - \bar{x}*\bar{y}$$

:::

::: {.column width="77%"}

Su signo nos indica la [**direcci√≥n de la dependencia**]{.hl-orange}: si es positiva, la relaci√≥n ser√° creciente (X crece, Y crece); negativa, relaci√≥n decreciente.

:::
::::

. . .

:::: columns
::: {.column width="80%"}

Como toda varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-orange} de los datos: necesitamos [**estandarizar la covarianza**]{.hl-orange} con el conocido [**coeficiente correlaci√≥n lineal**]{.hl-orange} ([**siempre entre -1 y 1**]{.hl-orange})

* m√°s cerca de -1 o 1 ‚Üí relaci√≥n lineal m√°s fuerte
* m√°s cerca de 0 ‚Üí ausencia de relaci√≥n **LINEAL** ‚Üí la pendiente de la recta ser√≠a nula, ¬°no habr√≠a modelo dependiente de X!
:::

::: {.column width="20%"}

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

:::
::::


---

## Covarianza y correlaci√≥n

![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/correlaciones.jpg?raw=true)

* nube de puntos m√°s apretada a la recta ‚Üí m√°s cerca de -1 o 1
* nube de puntos m√°s distante de la recta ‚Üí m√°s cerca de 0 (incorreladas) ‚Üí ¬°pero no implica que exista relaci√≥n, solo que no es en torno a una recta! ([**incorrelaci√≥n no implica independencia, correlaci√≥n no implica causalidad**]{.hl-orange})

. . .

As√≠ que parecer√≠a razonable [**chequear antes con una visualizaci√≥n**]{.hl-orange} si la nube de puntos se distribuye en torno a una recta.

---

## Fiabilidad

2. ¬øC√≥mo cuantificar la [**fiabilidad de mi estimaci√≥n**]{.hl-orange}? Para responder deberemos de atender a dos preguntas

* ¬øCu√°nto [**acierta mi modelo en la muestra**]{.hl-orange} con la que ha sido entrenado? <span style="visibility: hidden;">‚Üí Para ello usaremos el conocido como [**$R^2$ o coeficiente de bondad de ajuste**]{.hl-purple}  (que en el caso que nos ocupa estar√° relacionado con esa correlaci√≥n) y que nos proporciona el [**% de informaci√≥n de Y que es explicada**]{.hl-purple} por el modelo</span>

&nbsp;

* En caso de poder generalizar mi modelo, ¬ø[**puedo generalizarlo sin l√≠mites**]{.hl-orange}? ¬øA qu√© rango de valores podr√≠a aplicarlo?


---

## Fiabilidad

2. ¬øC√≥mo cuantificar la [**fiabilidad de mi estimaci√≥n**]{.hl-orange}? Para responder deberemos de atender a dos preguntas

* ¬øCu√°nto [**acierta mi modelo en la muestra**]{.hl-orange} con la que ha sido entrenado? ‚Üí Para ello usaremos el conocido como [**$R^2$ o coeficiente de bondad de ajuste**]{.hl-purple}  (que en el caso que nos ocupa estar√° relacionado con esa correlaci√≥n) y que nos proporciona el [**% de informaci√≥n de Y que es explicada**]{.hl-purple} por el modelo

&nbsp;

* En caso de poder generalizar mi modelo, ¬ø[**puedo generalizarlo sin l√≠mites**]{.hl-orange}? ¬øA qu√© rango de valores podr√≠a aplicarlo?

---

## Fiabilidad

2. ¬øC√≥mo cuantificar la [**fiabilidad de mi estimaci√≥n**]{.hl-orange}? Para responder deberemos de atender a dos preguntas

* ¬øCu√°nto [**acierta mi modelo en la muestra**]{.hl-orange} con la que ha sido entrenado? ‚Üí Para ello usaremos el conocido como [**$R^2$ o coeficiente de bondad de ajuste**]{.hl-purple} (que en el caso que nos ocupa estar√° relacionado con esa correlaci√≥n) y que nos proporciona el [**% de informaci√≥n de Y que es explicada**]{.hl-purple} por el modelo

&nbsp;

* En caso de poder generalizar mi modelo, ¬ø[**puedo generalizarlo sin l√≠mites**]{.hl-orange}? ¬øA qu√© rango de valores podr√≠a aplicarlo? ‚Üí En caso de poder generalizarse, solo deber√≠a aplicar mi [**modelo a datos en el rango de los datos de aprendizaje**]{.hl-purple}. Si tengo un modelo para predecir la probabilidad de infarto con informaci√≥n de personas entre 18 y 40 a√±os, no ser√° fiable (aunque funcione perfecto en la muestra) para personas de 70 a√±os.

---

## Inferir a la poblaci√≥n

3. ¬øC√≥mo garantizar que el [**m√©todo lo podr√≠a extrapolar/inferir**]{.hl-orange} a otros individuos ajenos a la muestra?

Es importante advertir que en los modelos de aprendizaje estamos todo el rato saltando entre dos mundos: la [**muestra**]{.hl-orange} (los datos que usamos para obtener una ecuaci√≥n de la recta) y la [**poblaci√≥n**]{.hl-orange} (si tomo otra muestra de la misma poblaci√≥n...¬øfuncionar√° igual de bien o mal?)

. . .

Y lo √∫nico que vamos a poder disponer (de momento) es una m√©trica error, de lo bueno o malo qu√© es, en nuestra muestra. [**¬øC√≥mo saber si tendr√≠a sentido su generalizaci√≥n o solo sirve para nuestra muestra?**]{.hl-orange}

. . .

[**Consejo**]{.hl-blue}: siempre que puedas visualiza antes los datos


---

## Inferir a la poblaci√≥n

Vamos a considerar los valores num√©ricos de **4 datasets distintos**

```{r}
#| echo: false
library(tidyr)
anscombe_data <-
  tibble("x" = c(datasets::anscombe$x1, datasets::anscombe$x2,
                 datasets::anscombe$x3, datasets::anscombe$x4),
         "y" = c(datasets::anscombe$y1, datasets::anscombe$y2,
                 datasets::anscombe$y3, datasets::anscombe$y4),
         "dataset" = c(rep("dataset-1", 11), rep("dataset-2", 11),
                       rep("dataset-3", 11), rep("dataset-4", 11)))
anscombe_data |> 
  summarise("mean_x" = mean(x), "mean_y" = mean(y),
            "cor_xy" = cor(x, y), .by = dataset)
```

Sabiendo que la [**correlaci√≥n**]{.hl-orange} est√° intrinsecamente relacionada con la calidad de nuestro modelo de recta, ¬øen cu√°l de ellos crees que funcionar√° mejor? ¬øEn cu√°l de ellos tendr√° sentido generalizarlo?

---

## Visualiza, siempre visualiza


:::: columns
::: {.column width="80%"}

```{r}
#| code-fold: true
ggplot(anscombe_data, aes(x = x, y = y, color = dataset)) +
  geom_point(size = 3.5, alpha = 0.65) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_manual(values = c("#2f357c", "#b0799a",
                                "#f5bb50", "#ada43b")) +
  facet_wrap(~dataset, nrow = 2) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

:::

::: {.column width="20%"}

&nbsp;

dataset 2 y 4: el modelo ha [**funcionado bien SOLO EN ESA MUESTRA**]{.hl-red}, ¬°pero ha sido casualidad, no tiene sentido generalizarlo!

:::
::::

---

## Visualiza, siempre visualiza


```{r}
#| code-fold: true
ggplot(datasauRus::datasaurus_dozen |>
         filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 3.5, alpha = 0.65, show.legend = FALSE) +
  geom_smooth(method = "lm", se = FALSE, show.legend = FALSE) +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  facet_wrap(~dataset, nrow = 4) +
  theme_minimal()
```

Todos tienen exactamente la misma media y correlaci√≥n.

---

##  Caso pr√°ctico

Vamos a poner en pr√°ctica lo aprendido usando un [**software estad√≠stico como R**]{.hl-orange} con el siguiente dataset

:::: columns
::: {.column width="56%"}

```{r}
library(readr)
datos <- read_csv(file = "./wine.csv")
datos
```

[**El objetivo: predecir el precio**]{.hl-orange}


:::

::: {.column width="44%"}

::: {.smaller-un-poquito-text}


El dataset est√° formado por 27 observaciones (cosechas de vino rojo Burdeos) y 7 variables

* `Year`, `Age`: a√±o de la cosecha y n√∫mero de a√±os en barrica.
* `Price`: precio en 1990-1991 (**escala log**)
* `WinterRain`: lluvia (en mm) que cay√≥ ese a√±o en invierno.
* `AGST`: crecimiento medio de la temperatura (en grados Celsius) durante la temporada.
* `HarvestRain`: lluvia (en mm) que cay√≥ ese a√±o durante la cosecha.
* `FrancePop`: poblaci√≥n (miles de habitantes) de Francia.

:::
:::
::::


---

## Caso pr√°ctico


Para predecir el precio vamos a usar (de momento) una [**regresi√≥n lineal univariante**]{.hl-orange}, donde $Y = precio$ y deberemos elegir la predictora $X$ m√°s apropiada. ¬øC√≥mo saber [**la predictora que va a tener mayor relaci√≥n lineal con $Y$**]{.hl-orange}?

. . .

:::: columns
::: {.column width="25%"}

```{r}
library(corrr)
datos |>
  correlate() |>
  select(term, Price)
```

:::

::: {.column width="37%"}

```{r}
#| echo: false
#| fig-width: 6.5
library(corrplot)
datos |> cor() |> corrplot()
```

```{r}
#| eval: false
library(corrplot)
datos |> cor() |> corrplot()
```

:::

::: {.column width="38%"}

```{r}
#| echo: false
#| fig-width: 6.5
library(GGally)
ggpairs(datos) + theme_minimal()
```

```{r}
#| eval: false
library(GGally)
ggpairs(datos) + theme_minimal()
```

:::
::::

---

## Caso pr√°ctico

Una vez que hemos decidido que predictora usaremos vamos a plantear el [**modelo univariante**]{.hl-orange}

$$Price = \beta_0 + \beta_1*AGST + \varepsilon \quad \text{¬øc√≥mo hacer en R el ajuste para obtener } \widehat{\beta}_0,~\widehat{\beta}_1 \text{?}$$

. . .

:::: columns
::: {.column width="50%"}

`lm(data = ., formula = y ~ .)` indicando los datos, la variable objetivo y las predictoras

```{r}
#| eval: false
library(broom)
fit <- lm(data = datos, formula = Price ~ AGST)
fit |> tidy()
```

:::

::: {.column width="50%"}

&nbsp;

&nbsp;

```{r}
#| echo: false
library(broom)
ajuste <- lm(data = datos, formula = Price ~ AGST)
ajuste |> tidy()
```

:::

::::


. . .

::: {.smaller-un-poquito-text}


* `Estimate` proporciona la estimaci√≥n de los par√°metros

  - $\widehat{\beta}_0=$ `r round(ajuste$coefficients[1], 3)`: predicci√≥n del precio (escala log) cuando $AGST = 0$ (es decir, si $AGST = 0$, entonces precio es de $0.0288 = exp(-3.547)$)
  - $\widehat{\beta}_1=$ `r round(ajuste$coefficients[2], 3)`: lo que aumenta el precio (escala log) por cada grado de aumento (es decir, por cada grado que aumenta la temperatura el precio se incrementa en escala logar√≠tmica $0.6426$, es decir, el precio se multiplica por $exp(0.6426) = 1.901$ --> aumenta un 90.1%)

:::

---

## Evaluaci√≥n: bondad de ajuste

```{r}
ajuste |> summary()
```

La salida nos proporciona muchos m√°s elementos que de momento no sabemos interpretar, pero si `Multiple R-squared`:  0.4456

Ese valor representa el $R^2$ de que ya hemos hablado, conocido como [**coeficiente de bondad de ajuste**]{.hl-orange}. ¬øQu√© es?

---

## Evaluaci√≥n: bondad de ajuste

Dicho coeficiente [**cuantifica el acierto del modelo**]{.hl-orange}. Y para ello la primera pregunta que deber√≠amos hacernos es: ¬øcu√°l es el **objetivo**? ¬øQu√© nos gustar√≠a que pasase si fuese perfecto?

. . .

Si construimos un **modelo predictivo para $Y$**, lo ideal ser√≠a que nuestro modelo puede capaz de [**capturar/explicar toda la variabilidad respecto a la media de Y**]{.hl-orange} (por qu√© sube cuando sube, por qu√© baja, etc), y siempre [**informaci√≥n = varianza**]{.hl-blue}

. . .

* [**Informaci√≥n de $Y$**]{.hl-orange} (info a modelizar): $s_{y}^2 = \frac{1}{n} \sum_{i=1}^{n} \left(y_i - \bar{y} \right)^2$

* [**Informaci√≥n de $\widehat{Y}$**]{.hl-orange} (info modelizada): $s_{\widehat{y}}^2 = \frac{1}{n} \sum_{i=1}^{n} \left(\widehat{y}_i - \bar{y} \right)^2$

* [**Informaci√≥n de $\widehat{\varepsilon}$**]{.hl-orange} (info no modelizada): $s_{r}^2 = \frac{1}{n} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$



. . .

Vamos a construir un [**ratio de info explicada**]{.hl-orange} ([**bondad de ajuste**]{.hl-orange} o $R^2$)

$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{\widehat{y}}^2}{s_{y}^2}  \in [0,1], \quad 
\text{si se acerca 1 -> explica casi todo lo explicable}$$


---

## Evaluaci√≥n: bondad de ajuste

Dicho coeficiente [**cuantifica el acierto del modelo**]{.hl-orange}. Y para ello la primera pregunta que deber√≠amos hacernos es: ¬øcu√°l es el **objetivo**? ¬øQu√© nos gustar√≠a que pasase si fuese perfecto?

Si construimos un **modelo predictivo para $Y$**, lo ideal ser√≠a que nuestro modelo puede capaz de [**capturar/explicar toda la variabilidad respecto a la media de Y**]{.hl-orange} (por qu√© sube cuando sube, por qu√© baja, etc), y siempre [**informaci√≥n = varianza**]{.hl-blue}


:::: columns
::: {.column width="67%"}



* [**Informaci√≥n de $Y$**]{.hl-orange} (info a modelizar): $s_{y}^2 = \frac{1}{n} \sum_{i=1}^{n} \left(y_i - \bar{y} \right)^2$

* [**Informaci√≥n de $\widehat{Y}$**]{.hl-orange} (info modelizada): $s_{\widehat{y}}^2 = \frac{1}{n} \sum_{i=1}^{n} \left(\widehat{y}_i - \bar{y} \right)^2$

* [**Informaci√≥n de $\widehat{\varepsilon}$**]{.hl-orange} (info no modelizada): $s_{r}^2 = \frac{1}{n} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$

:::

::: {.column width="5%"}

:::

::: {.column width="28%"}

::: {.smaller-un-poquito-text}


$$s_{y}^2 = s_{\widehat{y}}^2 + s_{r}^2$$

(para modelos lineales)

:::
:::

::::

Vamos a construir un [**ratio de info explicada**]{.hl-orange} ([**bondad de ajuste**]{.hl-orange} o $R^2$)

$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{\widehat{y}}^2}{s_{y}^2}  \in [0,1], \quad 
\text{si se acerca 1 -> explica casi todo lo explicable}$$


---

## Fase de evaluaci√≥n

$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{\widehat{y}}^2}{s_{y}^2}  \in [0,1], \quad s_{y}^2 = s_{\widehat{y}}^2 + s_{r}^2$$

Esa descomposici√≥n de la varianza (conocido como descomposici√≥n ANOVA) nos sirve para obtener la siguiente f√≥rmula en modelos lineales


$$R^2 = \frac{\text{info modelizada}}{\text{info a modelizar}} = \frac{s_{\widehat{y}}^2}{s_{y}^2} =  \frac{s_{y}^2 - s_{r}^2}{s_{y}^2} =   1 - \frac{s_{r}^2}{s_{y}^2}  \in [0,1]$$

Adem√°s en el [**caso univariante**]{.hl-orange} tenemos que $R^2 = r_{xy}^2$ (correlaci√≥n al cuadrado).

. . .


Un gran poder conlleva una gran responsabilidad, y es que el [**abuso de usar solo el $R^2$ para evaluar nuestro modelo**]{.hl-orange} puede llevarnos a situaciones dif√≠ciles de manejar.

---

## Problemas con el R2

Vamos a [**simular nuestros propios**]{.hl-orange} datos para entenderlo mejor

1. Simulamos una [**muestra de una predictora $X$**]{.hl-orange} de tama√±o $n = 100$ que siga una $N(2, \sigma = 0.5)$

```{r}
set.seed(12345)
x <- rnorm(n = 100, mean = 2, sd = 0.5)
```

. . .

2. Simulamos [**dos muestras de errores $\varepsilon$**]{.hl-orange} que sigan $N(0, \sigma = 0.5)$ y  $N(2, \sigma = 2.5)$, resp.


:::: columns
::: {.column width="50%"}

```{r}
error_1 <- rnorm(n = 100, mean = 0, sd = 0.5)
```

:::

::: {.column width="50%"}

```{r}
error_2 <- rnorm(n = 100, mean = 0, sd = 2.5)
```

:::

::::

. . .

3. Vamos a construir un dataset con $X$ e $Y$ donde la [**parte determin√≠stica (lo que podemos modelizar) sea la misma**]{.hl-orange} $E[Y|X] = -3 + 2X$, solo cambiando el ruido

:::: columns
::: {.column width="50%"}


```{r}
#| eval: false
datos_1 <- tibble("x" = x, "error" = error_1,
                  "y" = -3 + 2*x + error)
datos_1
```

```{r}
#| echo: false
datos_1 <- tibble("x" = x, "error" = error_1,
                  "y" = -2 + 1.5*x + error)
datos_1 |> slice(1:4)
```

:::

::: {.column width="50%"}

```{r}
#| eval: false
datos_2 <- tibble("x" = x, "error" = error_2,
                  "y" = -3 + 2*x + error)
datos_2
```

```{r}
#| echo: false
datos_2 <- tibble("x" = x, "error" = error_2,
                  "y" = -2 + 1.5*x + error)
datos_2 |> slice(1:4)
```

:::

::::


---

## Problemas con el R2


:::: columns
::: {.column width="50%"}

```{r}
#| eval: false
ggplot(datos_1, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
#| echo: false
ggplot(datos_1, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(-4, 6)) +
  theme_minimal()
```

:::

::: {.column width="50%"}

```{r}
#| eval: false
ggplot(datos_2, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
#| echo: false
ggplot(datos_2, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(-4, 6)) +
  theme_minimal()
```

:::

::::


. . .

¬°Es casi la [**misma recta de regresi√≥n**]{.hl-blue}! La parte que se puede modelizar es la misma pero uno tiene $R^2 = 0.76$ (modelo explica el 76% de la info de $Y$) y el otro $R^2 = 0.09$

:::: columns
::: {.column width="50%"}

```{r}
cor(datos_1$x, datos_1$y)^2
```

:::

::: {.column width="50%"}

```{r}
cor(datos_2$x, datos_2$y)^2
```

:::

::::

---

## Generalizar: inferencia

No solo es que el $R^2$ va a depender intr√≠nsecamente del contexto (m√°s o menos ruidoso) sino que adem√°s solo nos habla de [**c√≥mo funciona el modelo en la muestra de entrenamiento**]{.hl-orange}. ¬øQu√© pasar√≠a si yo tomase otra muestra?

. . .

::: incremental
* Muestra 1 ‚Üí par√°metros estimados $\left(\widehat{\beta}_{0}^{1}, \widehat{\beta}_{1}^{1}\right)$

* Muestra 2 ‚Üí par√°metros estimados $\left(\widehat{\beta}_{0}^{2}, \widehat{\beta}_{1}^{2}\right)$


* Muestra M ‚Üí par√°metros estimados $\left(\widehat{\beta}_{0}^{M}, \widehat{\beta}_{1}^{M}\right)$

:::

. . .

Tendremos dos realizaciones de dos variables aleatorias $\left(\widehat{\beta}_{0}^{1}, \ldots, \widehat{\beta}_{0}^{M}\right)$ y $\left(\widehat{\beta}_{1}^{1}, \ldots, \widehat{\beta}_{1}^{M}\right)$.

¬øPodemos saber algo de su [**distribuci√≥n**]{.hl-orange}? ¬øVan a **cambiar** mucho entre cada muestra? (si cambiasen mucho entre muestra y muestra, ser√≠a una [**estimaci√≥n poco precisa**]{.hl-orange})

---

## Generalizar: inferencia

Vamos a de nuevo a simular nuestros propios datos

1. Simulamos una muestra de tama√±o $n = 500$ de una predictora $X \sim N(\mu=3, \sigma = 1)$, un ruido $\varepsilon \sim N(\mu=0, \sigma = 0.3)$ y una variable objetivo aleatoria $Y$ bajo la hip√≥tesis de que $Y$ puede ser explicada por $X$ mediante la recta $\beta_0 = -2$ y $\beta_1 = 5$.

```{r}
set.seed(12345)
n <- 500
x <- rnorm(n = n, mean = 3, sd = 1)
eps <- rnorm(n = n, mean = 0, sd = 0.3)
y <- -2 + 5*x + eps
datos <- tibble("x" = x, "y" = y)
```

. . .

2. Calculamos (manualmente o con `lm()`) los valores estimados $\widehat{\beta}_0$ y $\widehat{\beta}_1$

```{r}
ajuste <- lm(data = datos, formula = y ~ x)
betas <- tibble("beta_0" = ajuste$coefficients[1], "beta_1" = ajuste$coefficients[2])
betas
```

---

## Generalizar: inferencia


3. El proceso anterior (x --> errores --> y --> estimar betas) lo vamos a meter **dentro un bucle de 2000 iteraciones**, guard√°ndonos las estimaciones $\widehat{\beta}_0$ y $\widehat{\beta}_1$ (`bind_rows()` concatena filas)

```{r}
set.seed(12345)
iteraciones <- 2000
betas <- tibble()
for (i in 1:iteraciones) {
  
  x <- rnorm(n = n, mean = 3, sd = 1)
  eps <- rnorm(n = n, mean = 0, sd = 0.3)
  y <- -2 + 5*x + eps
  datos <- tibble("x" = x, "y" = y)

  ajuste <- lm(data = datos, formula = y ~ x)
  betas <-
    betas |> 
    bind_rows(tibble("iteracion" = i, "beta_0" = ajuste$coefficients[1], "beta_1" = ajuste$coefficients[2]))
}
betas
```

---

## Generalizar: inferencia


```{r}
#| code-fold: true
ggplot(betas |>
         select(-iteracion) |> 
         pivot_longer(cols = everything(), names_to = "beta",
                      values_to = "values")) +
  geom_density(aes(x = values, color = beta, fill = beta),
               alpha = 0.6) +
  geom_vline(data = tibble("x" = c(-2, 5), "beta" = c("beta_0", "beta_1")),
             aes(xintercept = x, color = beta)) +
  geom_vline(data =
               tibble("beta" =  c("beta_0", "beta_1"),
                      "mean_beta" = c(mean(betas$beta_0), mean(betas$beta_1))),
             aes(xintercept = mean_beta, group = beta),
                 color = "#414141", linetype = "dotted") +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  facet_wrap(~beta, scales = "free_x") +
  labs(title = "Betas a partir de simular datos",
       subtitle = "(lo que pasar√≠a si se pudiese tomar una muestra tras otra)") +
  theme_minimal()
```

::: {.smaller-un-poquito-text}


Las l√≠neas verticales de colores representan los valores reales $\beta_0$ y $\beta_1$ a estimar, y la l√≠nea discontinua gris la estimaci√≥n media tras 2000 iteraciones: la [**diferencia entre ambas se conoce como sesgo**]{.hl-orange} (diferencia promedio entre el estimador y lo estimado)

:::

---

## Generalizar: inferencia


```{r}
#| code-fold: true
ggplot(betas |>
         select(-iteracion) |> 
         pivot_longer(cols = everything(), names_to = "beta",
                      values_to = "values")) +
  geom_density(aes(x = values, color = beta, fill = beta),
               alpha = 0.6) +
  geom_vline(data = tibble("x" = c(-2, 5), "beta" = c("beta_0", "beta_1")),
             aes(xintercept = x, color = beta)) +
  geom_vline(data =
               tibble("beta" =  c("beta_0", "beta_1"),
                      "mean_beta" = c(mean(betas$beta_0), mean(betas$beta_1))),
             aes(xintercept = mean_beta, group = beta),
                 color = "#414141", linetype = "dotted") +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  facet_wrap(~beta, scales = "free_x") +
  labs(title = "Betas a partir de simular datos",
       subtitle = "(lo que pasar√≠a si se pudiese tomar una muestra tras otra)") +
  theme_minimal()
```

::: {.smaller-un-poquito-text}

La concentraci√≥n de la campana nos cuantifica la [**varianza de los estimadores**]{.hl-orange}: un valor elevado implicar√≠a poca precisi√≥n (seg√∫n que muestra tomes, cambian mucho las conclusiones).

:::

---

## Balance sesgo-varianza



:::: columns
::: {.column width="50%"}

::: {.smaller-muestra-poblacion-text}


* El [**sesgo**]{.hl-orange} en una estimaci√≥n es un [**error sistem√°tico**]{.hl-orange}. 

* La [**varianza**]{.hl-orange} de los estimadores cuantificar√°n su [**precisi√≥n**]{.hl-orange}: cu√°nto var√≠a si tom√°semos otra muestra


* El error en estimar $\widehat{Y}$ se puede descomponer

$$\begin{eqnarray}MSE \left(\widehat{Y} \right) &=& \left( E \left[  \widehat{Y}\right] - Y \right)^2 + Var \left[ \widehat{Y} \right] + ruido \nonumber \\ &=& sesgo^2 + varianza + ruido\end{eqnarray}$$


:::

:::

::: {.column width="4%"}

:::

::: {.column width="46%"}


![](https://github.com/dadosdelaplace/docencia/blob/main/supervisado-datascience/diapos/img/bias_varianc_tradeoff.jpg?raw=true){width=340px}

:::

::::


. . .

:::: columns
::: {.column width="50%"}

::: {.smaller-muestra-poblacion-text}


* Si **bajamos el sesgo** sube la varianza; si **baja la varianza** sube el sesgo: [**balance sesgo-varianza**]{.hl-orange}

  - [**Bajoajuste (underfitting)**]{.hl-orange}: modelos muy simples proporcionan un **sesgo muy grande** y **poca varianza** (predicci√≥n siempre igual)
  - [**Sobreajuste (overfitting)**]{.hl-orange}: modelos muy complicados proporcionan un **sesgo bajo** pero una **gran varianza** para cada intento (memoriza)


:::

:::

::: {.column width="4%"}

:::

::: {.column width="46%"}


![](https://datascience.recursos.uoc.edu/wp-content/uploads/2022/09/PID_00275826_38.jpg){width=300px}

:::

::::



---


## Balance sesgo-varianza


```{r}
#| echo: false
ggplot(betas |>
         select(-iteracion) |> 
         pivot_longer(cols = everything(), names_to = "beta",
                      values_to = "values")) +
  geom_density(aes(x = values, color = beta, fill = beta),
               alpha = 0.6) +
  geom_vline(data = tibble("x" = c(-2, 5), "beta" = c("beta_0", "beta_1")),
             aes(xintercept = x, color = beta)) +
  geom_vline(data =
               tibble("beta" =  c("beta_0", "beta_1"),
                      "mean_beta" = c(mean(betas$beta_0), mean(betas$beta_1))),
             aes(xintercept = mean_beta, group = beta),
                 color = "#414141", linetype = "dotted") +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  facet_wrap(~beta, scales = "free_x") +
  labs(title = "Betas a partir de simular datos",
       subtitle = "(lo que pasar√≠a si se pudiese tomar una muestra tras otra)") +
  theme_minimal()
```

Si te fijas en nuestro caso los [**estimadores m√≠nimos cuadrados son insesgados**]{.hl-orange}, es decir, $E[\widehat{\beta}] = \beta$. ¬øY su varianza? ¬øY su distribuci√≥n?

---


## Generalizar: inferencia


Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$ pero la regresi√≥n lineal, como todo modelo param√©trico, [**necesita de hip√≥tesis**]{.hl-orange}. ¬øPara qu√© necesitar√≠amos [**hip√≥tesis**]{.hl-orange} entonces?

. . .

Hasta ahora  lo √∫nico que hemos podido realizar es una [**estimaci√≥n puntual**]{.hl-orange} de los par√°metros, pero dado que dichos estimadores ser√°n variables aleatorias, necesitaremos realizar [**inferencia estad√≠stica**]{.hl-orange} sobre ellos para poder extrapolar/inferior los resultados a la poblaci√≥n.

. . .

En el caso de la regresi√≥n lineal univariante pediremos [**4 hip√≥tesis**]{.hl-orange}

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante, sin depender de $x$**]{.hl-orange}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$.

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**


---


## Generalizar: inferencia


Las 4 hip√≥tesis se pueden [**resumir de manera te√≥rica**]{.hl-orange} en $Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$

. . .

Las hip√≥tesis nos permiten decir que los [**par√°metros estimados siguen una distribuci√≥n normal**]{.hl-orange} de [**media el par√°metro**]{.hl-orange}  a estimar y de [**varianza el conocido como standard error**]{.hl-orange}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

En el caso univariante tenemos que la [**varianza de nuestros par√°metros**]{.hl-orange} se define como

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\bar{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-blue}: ambos son [**estimadores insesgados**]{.hl-orange} ya que  $E \left[ \widehat{\beta}_j \right] = \beta_j$

¬øPero de qu√© depende la [**precisi√≥n (varianza de los estimadores)**]{.hl-orange}?

---

## Generalizar: inferencia

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\bar{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

* [**Precisi√≥n vs tama√±o muestral**]{.hl-blue}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, [**a m√°s datos, mayor precision**]{.hl-green} (menos varianza tendr√°n los estimadores)


* [**Precisi√≥n vs var residual**]{.hl-blue}: m√°s varianza del error, mayor varianza de los par√°metros, es decir, [**m√°s ruido implicar√° menos precisi√≥n**]{.hl-red}.


* [**Precisi√≥n vs varianza de X**]{.hl-blue}: m√°s varianza de la predictora $s_{x}^2$, menos varianza de los par√°metros, es decir, cu√°nta [**m√°s informaci√≥n (varianza) contenga nuestra tabla, mayor precisi√≥n**]{.hl-green}.

* [**Precisi√≥n vs media X**]{.hl-blue}: solo afecta $\beta_0$, cuya [**precisi√≥n decrece cuando la media aumenta**]{.hl-red}.

. . .

¬øY [**$\sigma_{\varepsilon}^2$ qu√© es**]{.hl-orange}?


---

## Generalizar: inferencia

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\bar{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

El valor $\sigma_{\varepsilon}^{2}$ representa la [**varianza residual poblacional**]{.hl-orange} (varianza poblacional del error te√≥rico), la cual es [**desconocida**]{.hl-orange} ya que solo contamos con una estimaci√≥n muestral de dicho error: dicho valor ser√° [**sustituido por su estimador insesgado**]{.hl-orange}  (donde $p$ es el n√∫mero de variables)


$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 := \text{Res standard error}^2$$

. . .


$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad \widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\bar{x}^2}{s_{x}^2} \right], \quad \widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$$


---

## Inferencia de los par√°metros


```{r}
ajuste |> summary()
```

::: {.smaller-muestra-poblacion-text}

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)$ (la estimaci√≥n de la desviaci√≥n t√≠pica de los par√°metros)


* `t value`: es el estad√≠stico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (bajo la hip√≥tesis nula $\beta_j = 0$)

* `Pr(>|t|)`: representa un p-valor del contraste $H_0:~\beta_j = 0$


* `Residual standard error`: estimador insesgado de la desv t√≠pica residual ($\widehat{\sigma_{\varepsilon}}$)

:::

---

## Inferencia de los par√°metros


```{r}
ajuste |> summary()
```

Si reflexionamos sobre lo que esto significa: [**si se cumplen una serie de hip√≥tesis**]{.hl-orange} podemos [**conocer la distribuci√≥n de los par√°metros**]{.hl-orange}, lo que nos permite inferior los resultados a la poblaci√≥n y, adem√°s, nos permite [**simular su valor sin necesidad de generar muestras en bucle**]{.hl-orange}

---

## Inferencia de los par√°metros

¬øRecuerdas la simulaci√≥n anterior? Hab√≠amos repetido el proceso de ajuste **dentro un bucle de 2000 iteraciones**, simulando en cada una de ellas una muestra de tama√±o $n = 500$ de $X \sim N(\mu=3, \sigma = 1)$, $\varepsilon \sim N(\mu=0, \sigma = 0.3)$ y una variable objetivo aleatoria $Y$ bajo la hip√≥tesis de que $Y$ puede ser explicada por $X$ mediante la recta $\beta_0 = -2$ y $\beta_1 = 5$.


```{r}
#| echo: false
set.seed(12345)
iteraciones <- 2000
betas <- tibble()
for (i in 1:iteraciones) {
  
  x <- rnorm(n = n, mean = 3, sd = 1)
  eps <- rnorm(n = n, mean = 0, sd = 0.3)
  y <- -2 + 5*x + eps
  datos <- tibble("x" = x, "y" = y)

  ajuste <- lm(data = datos, formula = y ~ x)
  betas <-
    betas |> 
    bind_rows(tibble("iteracion" = i, "beta_0" = ajuste$coefficients[1], "beta_1" = ajuste$coefficients[2]))
}

ggplot(betas |>
         select(-iteracion) |> 
         pivot_longer(cols = everything(), names_to = "beta",
                      values_to = "values")) +
  geom_density(aes(x = values, color = beta, fill = beta),
               alpha = 0.6) +
  geom_vline(data = tibble("x" = c(-2, 5), "beta" = c("beta_0", "beta_1")),
             aes(xintercept = x, color = beta)) +
  geom_vline(data =
               tibble("beta" =  c("beta_0", "beta_1"),
                      "mean_beta" = c(mean(betas$beta_0), mean(betas$beta_1))),
             aes(xintercept = mean_beta, group = beta),
                 color = "#414141", linetype = "dotted") +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  facet_wrap(~beta, scales = "free_x") +
  labs(title = "Betas a partir de simular datos",
       subtitle = "(lo que pasar√≠a si se pudiese tomar una muestra tras otra)") +
  theme_minimal()
```

. . .

Y aqu√≠ viene la magia: si se cumplen las hip√≥tesis, [**¬°basta con que hagas una ya que puedes usar su distribuci√≥n!**]{.hl-orange}

---

## Inferencia de los par√°metros

1. Simulamos muestra de $n = 500$ de $X \sim N(\mu=3, \sigma = 1)$, $\varepsilon \sim N(\mu=0, \sigma = 0.3)$ y una variable objetivo aleatoria $Y$ bajo la hip√≥tesis de que $Y$ puede ser explicada por $X$ mediante la recta $\beta_0 = -2$ y $\beta_1 = 5$. Calculamos (manualmente o con `lm()`) los valores estimados $\widehat{\beta}_0$ y $\widehat{\beta}_1$

```{r}
set.seed(12345)
n <- 500
x <- rnorm(n = n, mean = 3, sd = 1)
eps <- rnorm(n = n, mean = 0, sd = 0.3)
y <- -2 + 5*x + eps
datos <- tibble("x" = x, "y" = y)
ajuste <- lm(data = datos, formula = y ~ x)
ajuste |> broom::tidy()
```

. . .

2. Si se cumplen hip√≥tesis $\widehat{\beta}_0 \sim N \left(-1.99, \sigma = 0.0443 \right)$ y $\widehat{\beta}_1 \sim N \left(5, \sigma = 0.0137 \right)$. Simulamos muestras de tama√±o 2000

:::: columns
::: {.column width="60%"}

```{r}
beta_0 <- rnorm(n = 2000, mean = -1.99, sd = 0.0443)
beta_1 <- rnorm(n = 2000, mean = 5, sd = 0.0137)
```

:::
::::

---

## Inferencia de los par√°metros

3. [**Comparamos la distribuci√≥n de los betas**]{.hl-orange} generados a partir de simular muestras y los simulados suponiendo que se cumplen las hip√≥tesis

```{r}
#| echo: false
#| fig-width: 12
set.seed(12345)
iteraciones <- 2000
betas <- tibble()
for (i in 1:iteraciones) {
  
  x <- rnorm(n = n, mean = 3, sd = 1)
  eps <- rnorm(n = n, mean = 0, sd = 0.3)
  y <- -2 + 5*x + eps
  datos <- tibble("x" = x, "y" = y)

  ajuste <- lm(data = datos, formula = y ~ x)
  betas <-
    betas |> 
    bind_rows(tibble("iteracion" = i, "beta_0" = ajuste$coefficients[1], "beta_1" = ajuste$coefficients[2]))
}

gg_1 <-
  ggplot(betas |>
         select(-iteracion) |> 
         pivot_longer(cols = everything(), names_to = "beta",
                      values_to = "values")) +
  geom_density(aes(x = values, color = beta, fill = beta),
               alpha = 0.6) +
  geom_vline(data = tibble("x" = c(-2, 5), "beta" = c("beta_0", "beta_1")),
             aes(xintercept = x, color = beta)) +
  geom_vline(data =
               tibble("beta" =  c("beta_0", "beta_1"),
                      "mean_beta" = c(mean(betas$beta_0), mean(betas$beta_1))),
             aes(xintercept = mean_beta, group = beta),
                 color = "#414141", linetype = "dotted") +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  facet_wrap(~beta, scales = "free_x") +
  labs(title = "Betas a partir de simular datos",
       subtitle = "(lo que pasar√≠a tomando una muestra tras otra)") +
  theme_minimal()

gg_2 <-
  ggplot(tibble("beta" = c(rep("beta_0", iteraciones), rep("beta_1", iteraciones)),
                "values" = c(beta_0, beta_1))) +
  geom_density(aes(x = values, color = beta, fill = beta),
               alpha = 0.6) +
  geom_vline(data = tibble("x" = c(-2, 5), "beta" = c("beta_0", "beta_1")),
             aes(xintercept = x, color = beta)) +
  geom_vline(data =
               tibble("beta" =  c("beta_0", "beta_1"),
                      "mean_beta" = c(mean(betas$beta_0), mean(betas$beta_1))),
             aes(xintercept = mean_beta, group = beta),
                 color = "#414141", linetype = "dotted") +
  MetBrewer::scale_color_met_d(palette_name = "Renoir") +
  MetBrewer::scale_fill_met_d(palette_name = "Renoir") +
  facet_wrap(~beta, scales = "free_x") +
  labs(title = "Betas simulados (a partir de su distribuci√≥n",
       subtitle = "(cuando se cumplen las hip√≥tesis)") +
  theme_minimal()

conjunto <-
  tibble("beta_1_sample" = betas$beta_1,
         "beta_1_sim" = beta_1)
gg_3 <-
  ggplot(conjunto) +
  geom_point(aes(x = beta_1_sim, y = beta_1_sample)) +
  scale_x_continuous(limits = c(4.9, 5.1)) +
  scale_y_continuous(limits = c(4.9, 5.1)) +
  coord_equal() +
  labs(x = "Beta_1 bajo la normal", y = "Beta_1 a partir de muestras simuladas") +
  theme_minimal()
library(patchwork)
gg_1 + gg_2 + gg_3
```

---

## Resumiendo

::: incremental

1. Visualizaci√≥n. Detecci√≥n de at√≠picos, ausentes y errores.

2. Selecci√≥n de la mejor predictora

3. Formulaci√≥n del modelo

4. Ajuste e interpretaci√≥n

5. Si se cumplen las hip√≥tesis podemos inferir los resultados y conclusiones a la poblaci√≥n. ¬øSon los par√°metros significativos?

6. Evaluaci√≥n: m√©tricas de error

7. Predicci√≥n de nuevos valores

:::

---

## Multivariante...

```{r}
starwars |> 
  select(name, height, mass, birth_year) |> 
  rename(age = birth_year) |> 
  slice(1:7)
```

¬øY qu√© sucede cuando [**usamos m√°s de una predictora**]{.hl-orange}?

. . .

Piensa como poder formular el modelo y la varianza residual en t√©rminos matriciales para obtener el estimador m√≠nimos cuadrados.



# MUCH√çSIMAS GRACIAS


---


## Regresi√≥n multivariante

De aqu√≠ en adelante llamaremos [**modelo multivariante**]{.hl-orange} a todo modelo en el que $p>1$ (es decir, tenemos m√°s de una variable predictora).

$$Y = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right)$$

tal que $E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0$.

. . .

En el caso del [**modelo lineal multivariante **]{.hl-orange}se traducir√° por tanto en

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j$$

El objetivo seguir√° siendo obtener la estimaci√≥n de los $\widehat{\beta}$ tal que minimicemos el error (la suma de errores al cuadrado).
 
--- 

## Formulaci√≥n matricial

Si tenemos $n$ observaciones y $p$ predictoras, su [**formulaci√≥n matricial te√≥rica**]{.hl-orange} la podemos expresar mediante la **matriz de dise√±o**

$$\mathbf{X} =\begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}, \quad Y_i =  \beta_0 + \displaystyle \sum_{j=1}^{p} \beta_j X_{ij} + \varepsilon_i$$

tal que


$$\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \beta_0 \\ \vdots \\ \beta_p \end{pmatrix}_{(p+1)\times1} + \begin{pmatrix} \varepsilon_1 \\  \vdots \\ \varepsilon_n \end{pmatrix}_{n\times1}$$

---

## Formulaci√≥n matricial

Dicho modelo te√≥rico ser√° estimado tal que

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$

---

## M√≠nimos cuadrados

Como pasaba en el modelo univariante, nuestro objetivo ser√° encontrar que [**vector de par√°metros minimiza la suma de errores al cuadrado**]{.hl-orange}

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)\right]^2$$


¬øC√≥mo quedar√≠a **matricialmente**?

. . .


$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = \varepsilon^{T} \varepsilon$$

por lo que, de todos los $\boldsymbol{\beta}$ posibles, el [**vector √≥ptimo de par√°metros**]{.hl_orange} $\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p\right)$ ser√° aquel que minimice $SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2}= \varepsilon^{T} \varepsilon$

---

## M√≠nimos cuadrados

Para hallar el m√≠nimo de la funci√≥n $SSE \left(\boldsymbol{\beta} \right)$ (donde $\boldsymbol{\beta}$ es el argumento) calcularemos la derivada respecto a $\boldsymbol{\beta}$ e igualaremos a 0

. . .

Teniendo en cuenta que $\left(A B \right)^T = B^T A^T$

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} \nonumber \\  &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}}\right)^{T} + \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}}  \nonumber \\  &=&  \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}}\right)^{T} -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} \nonumber \\  &=&  \left( \left(\mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}}\right)^{T} - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X}\end{eqnarray}$$

---

## M√≠nimos cuadrados

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}} \right)^T -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X}\end{eqnarray}$$ 
¬øCu√°nto vale $\frac{\partial \left(  - \boldsymbol{\beta}^{T} A  \right)}{\partial \boldsymbol{\beta}}$ para una matriz $A$ de constantes cualesquiera?

. . .


$$\begin{eqnarray}\frac{\partial \left(  - \boldsymbol{\beta}^{T} A  \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial}{\partial \boldsymbol{\beta}}\left(-\beta_0, \ldots, -\beta_p\right)\begin{pmatrix} a_{11} & \ldots & a_{1q} \\ \vdots & \ddots & \vdots \\
a_{p1} & \ldots & a_{pq}
\end{pmatrix} = - \frac{\partial}{\partial \boldsymbol{\beta}} \left(\sum_{i=1}^{p} \beta_i a_{i1}, \ldots, \sum_{i=1}^{p} \beta_i a_{iq}\right) \nonumber \\ &=& - \begin{pmatrix} \frac{\partial}{\partial \beta_1} \sum_{i=1}^{p} \beta_i a_{i1} & \ldots & \frac{\partial}{\partial \beta_p} \sum_{i=1}^{p} \beta_i a_{i1} \\ \vdots & \ddots & \vdots \\
\frac{\partial}{\partial \beta_1} \sum_{i=1}^{p} \beta_i a_{iq} & \ldots & \frac{\partial}{\partial \beta_p} \sum_{i=1}^{p} \beta_i a_{iq}
\end{pmatrix} = - \begin{pmatrix} a_{11} & \ldots & a_{p1} \\ \vdots & \ddots & \vdots \\
a_{1q} & \ldots &  a_{pq}
\end{pmatrix} = -A^{T}\end{eqnarray}$$


---


## M√≠nimos cuadrados

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}} \right)^T -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X}\end{eqnarray}$$  

Usando dicho resultado tenemos por tanto que

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \left(\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left(  - \boldsymbol{\beta}^{T} \mathbf{X}^{T}  \right)}{\partial \boldsymbol{\beta}} \right)^T -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} \nonumber \\ &=& - \left(\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)\right)^T  -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} \nonumber \\ &=& -2\left(\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)\right)^T = 0 \end{eqnarray}$$ 

. . .

Eso es equivalente a decir que $\widehat{\boldsymbol{\beta}}$ ser√° aquel vector que

$$\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \widehat{\boldsymbol{\beta}} \right) = 0$$ 

---

## M√≠nimos cuadrados

$$\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \widehat{\boldsymbol{\beta}} \right) = 0$$ 

Despejando tenemos que

$$\mathbf{X}^{T} \mathbf{Y} - \mathbf{X}^{T}\mathbf{X} \widehat{\boldsymbol{\beta}}  = 0 \Rightarrow \widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T} \mathbf{Y} $$

. . .

[**IMPORTANTE**]{.hl-orange}: para que exista soluci√≥n (√∫nica) necesitamos que $\mathbf{X}^{T}\mathbf{X}$ sea invertible, es decir, que $\left| \mathbf{X}^{T}\mathbf{X} \right| \neq 0$. 


---

## Estimaci√≥n multivariante

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$

As√≠ la estimaci√≥n muestral ser√° $\widehat{\mathbf{Y}} = X \widehat{\boldsymbol{\beta}} = X\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T} \mathbf{Y}$ tal que $H:=X\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}$ se conoce
como [**hat matrix o matriz de proyecci√≥n**]{.hl-orange} (ya que hace que las estimaciones $\widehat{y}$ sean en realidad los valores y proyectados verticalmente sobre el plano de regresi√≥n ajustado).
  
  
